{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5236465,"sourceType":"datasetVersion","datasetId":3046745},{"sourceId":9603515,"sourceType":"datasetVersion","datasetId":5859144},{"sourceId":9606378,"sourceType":"datasetVersion","datasetId":5861159},{"sourceId":9606403,"sourceType":"datasetVersion","datasetId":5861179}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install pyvi\n# !pip install dask\n# !pip install num2words\n# !pip install spacy\n# !python -m spacy download en_core_web_sm\n\n# !pip install dask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- References:\n  1. Preprocessing: https://www.studocu.com/vn/document/truong-dai-hoc-cong-nghe-thong-tin-va-truyen-thong-viet-han/vi-dieu-khien/danang-nlp-pre-processing-vietnamese/72944823\n  2. https://huggingface.co/madha98/Text_Generation_LSTM/blob/main/automatic-text-generation-lstm5.ipynb\n  3. https://www.kaggle.com/code/prashant111/a-beginners-guide-to-dealing-with-text-data#4.6-Sentiment-Analysis--\n  5. https://www.kaggle.com/code/shivamb/beginners-guide-to-text-generation-using-lstms/notebook#3.-Dataset-preparation\n  6. https://nguyenvanhieu.vn/phan-loai-van-ban-tieng-viet/#ftoc-heading-4\n  7. https://github.com/trannguyenhan/preprocessing-data/blob/ipynb/preprocessing_data.ipynb\n  8. https://www.analyticsvidhya.com/blog/2022/02/explaining-text-generation-with-lstm/\n  9. https://www.researchgate.net/publication/321259272_Multi-channel_LSTM-CNN_model_for_Vietnamese_sentiment_analysis (đọc sau)","metadata":{}},{"cell_type":"code","source":"from pyvi import ViTokenizer\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport time\nimport string\nfrom num2words import num2words as n2w \nimport spacy \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport shutil\nfrom dask import delayed, compute\nfrom dask.bag import Bag","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T10:24:55.790137Z","iopub.execute_input":"2024-10-12T10:24:55.790819Z","iopub.status.idle":"2024-10-12T10:24:56.626777Z","shell.execute_reply.started":"2024-10-12T10:24:55.790775Z","shell.execute_reply":"2024-10-12T10:24:56.625475Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# TEST 1 file\n\n# Read toàn bộ nội dung trong file .txt (A)\ndef read_output_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.readlines()  \n    return content\n\noutput_file_path = '/kaggle/working/processed_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\n\nfile_content = read_output_file(output_file_path)\n\n# Dùng pandas để read file, bỏ qua các dòng lỗi ==> kết quả là phần header, title, footer (B)\ndf_4cogai = pd.read_csv(\n    output_file_path,\n    encoding='utf-8',\n    sep='\\t',\n    on_bad_lines='skip'\n)\n\n# Lấy các dòng từ (A) dưới dạng list\nlines_from_df = df_4cogai.astype(str).apply(lambda x: '\\t'.join(x), axis=1).tolist()\n\n# Lấy các dòng có trong A nhưng không có trong B. => Phần này em test với tầm 5 files thì thấy kết quả giữ lại phần nội dung, loại bỏ các phần như Chương 1, Chương 2, ... phần source ở dưới mỗi file.\n# => Vì không nghĩ ra được rule để clean những phần đó trong mỗi file .txt nên em dùng cách này, chắc là trick thôi\nlines_not_in_df = [line.strip() for line in file_content if line.strip() not in lines_from_df]\nfor line in lines_not_in_df:\n    print(line)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply for all files, clean header, chapter & footer content\n\ndef read_output_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.readlines()\n    return content\n\ndef process_file(file_path, output_folder):\n    try:\n        file_content = read_output_file(file_path)\n        \n        df = pd.read_csv(file_path, encoding='utf-8', sep='\\t', on_bad_lines='skip')\n        \n        lines_from_df = df.astype(str).apply(lambda x: '\\t'.join(x), axis=1).tolist()\n        \n        lines_not_in_df = [line.strip() for line in file_content if line.strip() not in lines_from_df]\n\n        output_file_path = os.path.join(output_folder, os.path.basename(file_path))\n        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n            for line in lines_not_in_df:\n                output_file.write(line + '\\n')\n\n    except Exception as e: # Try except để kiểm tra có file nào không apply được cách này không, báo lỗi để check tiếp\n        print(f\"Error processing {file_path}: {e}\")\n\ninput_folder = '/kaggle/input/10000-vietnamese-books/output'\noutput_folder = '/kaggle/working/cleaned_header_footer_books' # lưu lại 1 file để lưu trữ, e commit file Kaggle bị failed, nên bị mất file ở output session rồi\n\n# nếu chưa có folder sẵn thì tạo folder mới\nos.makedirs(output_folder, exist_ok=True)\n\n# Tính thời gian chạy\nstart_time = time.time()\n\nfile_list = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\nfor file_name in tqdm(file_list, desc=\"Processing files\"):\n    file_path = os.path.join(input_folder, file_name)\n    process_file(file_path, output_folder)\n\n# Tính thời gian chạy\nend_time = time.time()\nprint(f\"Processing completed in {end_time - start_time:.2f} seconds.\") # Thời gian chạy \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T07:08:22.098611Z","iopub.execute_input":"2024-10-12T07:08:22.099018Z","iopub.status.idle":"2024-10-12T07:10:26.201726Z","shell.execute_reply.started":"2024-10-12T07:08:22.098978Z","shell.execute_reply":"2024-10-12T07:10:26.200488Z"}},"outputs":[{"name":"stderr","text":"Processing files: 100%|██████████| 10415/10415 [02:04<00:00, 83.95it/s] ","output_type":"stream"},{"name":"stdout","text":"Processing completed in 124.09 seconds.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# Check random 1 file sau khi clean header & footer (4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt)\ndef read_output_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    return content\n\noutput_file_path = '/kaggle/working/cleaned_header_footer_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\n\nfile_content = read_output_file(output_file_path)\n\nprint(file_content[:100]) # Phần header và tên các chương ko còn\n\nprint(file_content[-100:]) # Phần footer chứa source,... đã clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T07:13:04.083099Z","iopub.execute_input":"2024-10-12T07:13:04.083784Z","iopub.status.idle":"2024-10-12T07:13:04.092304Z","shell.execute_reply.started":"2024-10-12T07:13:04.083742Z","shell.execute_reply":"2024-10-12T07:13:04.091373Z"}},"outputs":[{"name":"stdout","text":"\nLễ Giáng sinh sẽ không phải là lễ Giáng sinh nếu chúng ta không có quà. - Jo vừa lẩm bẩm vừa nằm dà\n hôn nhân hạnh phúc cũng đủ để kết thúc vui vẻ một câu chuyện từng trải qua những giờ phút đen tối.\n\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"import spacy\n# Tải mô hình SpaCy cho tiếng Anh\nnlp = spacy.load('en_core_web_sm') # dùng để clean tên riêng tiếng Anh xuất hiện trong các files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T06:01:41.654448Z","iopub.execute_input":"2024-10-12T06:01:41.655014Z","iopub.status.idle":"2024-10-12T06:01:45.394721Z","shell.execute_reply.started":"2024-10-12T06:01:41.654974Z","shell.execute_reply":"2024-10-12T06:01:45.393644Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# source: https://github.com/HienBM/sentiment_analysis_with_deep_learning/blob/main/Vietnamese_Proper_Nouns.txt\nvietnamese_proper_nouns_path = '/kaggle/input/d/quynhnhitran02/vietnamese-proper-nouns/Vietnamese_Proper_Nouns.txt' # clean tên riêng tiếng Việt xuất hiện trong các files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T06:28:44.165445Z","iopub.execute_input":"2024-10-12T06:28:44.166143Z","iopub.status.idle":"2024-10-12T06:28:44.170822Z","shell.execute_reply.started":"2024-10-12T06:28:44.166103Z","shell.execute_reply":"2024-10-12T06:28:44.169793Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"# Pre processing test với 1 file 4 cô con gái nhà bác sỹ March - Louisa May Alcott.tx\n\n# load_stopwords\ndef load_stopwords(filepath):\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return set(f.read().splitlines())\n\n# load_vietnamese_proper_nouns\ndef load_vietnamese_proper_nouns(filepath):\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return set(f.read().splitlines())\n\n# normalize_text\ndef normalize_text(text):\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text)  # Thay thế nhiều khoảng trắng thành một khaongr trắng\n    text = re.sub(r'[,.]', ' ', text)  # Thay thế dấu . và , bằng khoảng trắng (Để từ cuối cùng của câu và từ bắt đầu của câu không bị dính vào nhau)\n    text = re.sub(r'[^\\w\\s]', '', text)  # Xóa ký tự không phải chữ và không phải số \n    text = re.sub(r'(\\d+)', '<number>', text)  # Thay số bằng '<number>'\n    return text\n\n# remove_english_proper_nouns\ndef remove_english_proper_nouns(text):\n    doc = nlp(text)\n    filtered_tokens = [token.text for token in doc if not (token.ent_type_ == \"PERSON\")]\n    return ' '.join(filtered_tokens)\n\n# remove_vietnamese_proper_nouns\ndef remove_vietnamese_proper_nouns(text, proper_nouns):\n    for noun in proper_nouns:\n        text = text.replace(noun, '')\n    return text\n\n# normalize_vietnamese_text\ndef normalize_vietnamese_text(text):\n    text = ViTokenizer.tokenize(text)  # Token hóa văn bản\n    text = re.sub(r'\\u200B', '', text)  # Xóa ký tự không nhìn thấy \n    return text.replace(' ', '')  # Xóa khoảng trắng\n\n# Hàm chuyển đổi số thành chữ\ndef lemmatize_numbers(text):\n    return re.sub(r'(\\d+)', lambda x: n2w(int(x.group())), text)  # Chuyển số thành chữ\n\n# Hàm token hóa văn bản tiếng Việt\ndef tokenize_vietnamese_text(text):\n    return ViTokenizer.tokenize(text)\n    \n# Hàm xóa từ trùng lặp\ndef remove_duplicates(tokens):\n    seen = set()  # Tạo một set để theo dõi các từ đã gặp\n    unique_tokens = []\n    for token in tokens.split():\n        if token not in seen:\n            unique_tokens.append(token)\n            seen.add(token)\n    return ' '.join(unique_tokens)\n\n# Hàm tiền xử lý văn bản đầy đủ\ndef preprocess_text(text, stop_words, proper_nouns):\n    text = normalize_text(text)\n    text = remove_english_proper_nouns(text)  # Xóa tên riêng tiếng Anh\n    text = remove_vietnamese_proper_nouns(text, proper_nouns)  # Xóa tên riêng tiếng Việt\n    tokens = tokenize_vietnamese_text(text)  # Token hóa văn bản\n    tokens = [word for word in tokens.split() if word not in stop_words]  # Loại bỏ stopwords\n    tokens = remove_duplicates(' '.join(tokens))  # Loại bỏ từ trùng lặp\n    text = lemmatize_numbers(tokens)  # Chuyển đổi số thành chữ\n    return text\n\n# Hàm xử lý file văn bản\ndef process_text_file(input_file, output_file, stop_words, proper_nouns):\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)  # Tạo thư mục nếu chưa tồn tại\n    try:\n        with open(input_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        processed_text = preprocess_text(content, stop_words, proper_nouns)\n\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(processed_text)\n            \n    except Exception as e:\n        print(f\"Error processing {input_file}: {e}\")\n\n# Đường dẫn đến file đầu vào và đầu ra\ninput_file_path = '/kaggle/working/cleaned_header_footer_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\noutput_file_path = '/kaggle/working/test_preprocessing_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\nstopwords_path = '/kaggle/input/vietnamesestopwordstxt/vietnamese-stopwords.txt'\nvietnamese_proper_nouns_path = '/kaggle/input/d/quynhnhitran02/vietnamese-proper-nouns/Vietnamese_Proper_Nouns.txt'  # Đường dẫn đến file tên riêng tiếng Việt\n\n# Tải stopwords và tên riêng tiếng Việt\nstop_words = load_stopwords(stopwords_path)\nvietnamese_proper_nouns = load_vietnamese_proper_nouns(vietnamese_proper_nouns_path)\n\n# Thời gian bắt đầu\nstart_time = time.time()\n\n# Xử lý file\nwith tqdm(total=1, desc=\"Processing files\") as pbar:  # Khởi tạo tqdm cho toàn bộ quy trình\n    process_text_file(input_file_path, output_file_path, stop_words, vietnamese_proper_nouns)\n    pbar.update(1)  # Cập nhật tiến trình sau khi xử lý xong file\n\n# Thời gian kết thúc\nend_time = time.time()\nprint(f\"Processing completed in {end_time - start_time:.2f} seconds.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T07:15:02.439270Z","iopub.execute_input":"2024-10-12T07:15:02.441793Z","iopub.status.idle":"2024-10-12T07:15:13.074074Z","shell.execute_reply.started":"2024-10-12T07:15:02.441752Z","shell.execute_reply":"2024-10-12T07:15:13.072737Z"}},"outputs":[{"name":"stderr","text":"Processing files: 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]","output_type":"stream"},{"name":"stdout","text":"Processing completed in 9.41 seconds.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"# Hàm tải stopwords từ file\ndef load_stopwords(filepath):\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return set(f.read().splitlines())\n\n# Hàm đọc danh sách tên riêng tiếng Việt\ndef load_vietnamese_proper_nouns(filepath):\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return set(f.read().splitlines())\n\n# Hàm chuẩn hóa văn bản\ndef normalize_text(text):\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text)  # Thay thế nhiều khoảng trắng thành một\n    text = re.sub(r'[,.]', ' ', text)  # Thay thế dấu . và , bằng khoảng trắng\n    text = re.sub(r'[^\\w\\s]', '', text)  # Xóa ký tự không phải chữ và không phải số\n    text = re.sub(r'(\\d+)', '<number>', text)  # Thay số bằng '<number>'\n    return text\n\n# Hàm xóa tên riêng tiếng Anh\ndef remove_english_proper_nouns(text):\n    doc = nlp(text)\n    filtered_tokens = [token.text for token in doc if not (token.ent_type_ == \"PERSON\")]\n    return ' '.join(filtered_tokens)\n\n# Hàm xóa tên riêng tiếng Việt\ndef remove_vietnamese_proper_nouns(text, proper_nouns):\n    for noun in proper_nouns:\n        text = text.replace(noun, '')\n    return text\n\n# Hàm chuẩn hóa văn bản tiếng Việt\ndef normalize_vietnamese_text(text):\n    text = ViTokenizer.tokenize(text)  # Token hóa văn bản\n    text = re.sub(r'\\u200B', '', text)  # Xóa ký tự không nhìn thấy\n    return text.replace(' ', '')  # Xóa khoảng trắng\n\n# xóa các từ tiếng anh xuất hiện trong các văn bản: đại_học copenhagen đan mạch nguyên_bản hội nghiên_cứu đảo đông paris giáo tế california asia\n\n# chuyển số thành chữ\n# def lemmatize_numbers(text):\n#     return re.sub(r'(\\d+)', lambda x: n2w(int(x.group())), text)  # Chuyển số thành chữ\n\n# Dùng thư viện ViTokenizer để tokenize\ndef tokenize_vietnamese_text(text):\n    return ViTokenizer.tokenize(text)\n    \n# Hàm các xóa từ trùng lặp trong văn bản: ví dụ lễ giáng sinh, chúng ta,...\ndef remove_duplicates(tokens):\n    seen = set()  # tạo set để theo dõi các từ đã gặp\n    unique_tokens = []\n    for token in tokens.split():\n        if token not in seen:\n            unique_tokens.append(token)\n            seen.add(token)\n    return ' '.join(unique_tokens)\n\n# preprocess_text\ndef preprocess_text(text, stop_words, proper_nouns):\n    text = normalize_text(text)\n    text = remove_english_proper_nouns(text)  # Xóa tên riêng tiếng Anh\n    text = remove_vietnamese_proper_nouns(text, proper_nouns)  # Xóa tên riêng tiếng Việt\n    tokens = tokenize_vietnamese_text(text)  # Token hóa văn bản\n    tokens = [word for word in tokens.split() if word not in stop_words]  # Loại bỏ stopwords\n    tokens = remove_duplicates(' '.join(tokens))  # Loại bỏ từ trùng lặp\n    text = lemmatize_numbers(tokens)  # Chuyển đổi số thành chữ\n    return text\n\n# process_text_file\ndef process_text_file(input_file, output_file, stop_words, proper_nouns):\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)  # tạo folder để lưu kết quả nếu chưa tồn tại\n    try:\n        with open(input_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        processed_text = preprocess_text(content, stop_words, proper_nouns)\n\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(processed_text)\n            \n    except Exception as e:\n        print(f\"Error processing {input_file}: {e}\")\n\ninput_file_path = '/kaggle/working/cleaned_header_footer_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\noutput_file_path = '/kaggle/working/test_preprocessing_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\nstopwords_path = '/kaggle/input/vietnamesestopwordstxt/vietnamese-stopwords.txt'\nvietnamese_proper_nouns_path = '/kaggle/input/d/quynhnhitran02/vietnamese-proper-nouns/Vietnamese_Proper_Nouns.txt'  # Đường dẫn đến file tên riêng tiếng Việt\n\n# Tải stopwords và tên riêng tiếng Việt\nstop_words = load_stopwords(stopwords_path)\nvietnamese_proper_nouns = load_vietnamese_proper_nouns(vietnamese_proper_nouns_path)\n\n# Thời gian bắt đầu\nstart_time = time.time()\n\n# Xử lý file\nwith tqdm(total=1, desc=\"Processing files\") as pbar:  # dùng tqdm check progress \n    process_text_file(input_file_path, output_file_path, stop_words, vietnamese_proper_nouns)\n    pbar.update(1)  # cập nhật progress sau khi xử lý xong file\n\n# Thời gian kết thúc\nend_time = time.time()\nprint(f\"Processing completed in {end_time - start_time:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T07:19:55.461629Z","iopub.execute_input":"2024-10-12T07:19:55.462323Z","iopub.status.idle":"2024-10-12T07:20:04.886128Z","shell.execute_reply.started":"2024-10-12T07:19:55.462281Z","shell.execute_reply":"2024-10-12T07:20:04.884792Z"}},"outputs":[{"name":"stderr","text":"Processing files: 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]","output_type":"stream"},{"name":"stdout","text":"Processing completed in 9.39 seconds.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"# kiểm tra kết quả sau khi pre processing\n\nprocessed_4cogai_path = '/kaggle/working/test_preprocessing_books/4 cô con gái nhà bác sỹ March - Louisa May Alcott.txt'\n\nprocessed_4cogai = read_output_file(processed_4cogai_path)\n\nprint(processed_4cogai[:400])\n\nprint(processed_4cogai[-400:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T07:20:10.016796Z","iopub.execute_input":"2024-10-12T07:20:10.017372Z","iopub.status.idle":"2024-10-12T07:20:10.024598Z","shell.execute_reply.started":"2024-10-12T07:20:10.017328Z","shell.execute_reply":"2024-10-12T07:20:10.023471Z"}},"outputs":[{"name":"stdout","text":"lễ giáng_sinh chúng_ta quà jo lẩm_bẩm nằm thảm khủng_khiếp ta giàu meg thở_dài áo có_thể công_bằng con_gái xinh_đẹp đứa chẳng bé amy tiếp_lời bấy_giờ vọng góc may_mắn mẹ thân_yêu bốn chị_em hòa_hợp quotcó thể bao_giờ quot trận đánh miền nam quốc đối_lập đồng_hồ treo tường sáu hơ đôi lửa mòn_vẹt bảo mua ý_kiến đáp vỗ thưởng vui lắm giọng vang ngưỡng_cửa sung_sướng đón phụ_nữ dáng_vẻ dễ_thương khuôn\nkiện trấn hưng vợ_chồng mơ_ước vờivà ngỏ cầu_hôn ruộng rừng_núi bò bê dê đỏm_dáng cày ghế_dài nhẹn chụp đâu_đây đâytôi chứluôn diệu_kỳ xanh_rì rừng trang_trại vô_số con_con vịt đực heo xu chương_trình lúa đài cỏn_con laurentzvậy nhà_nông chính_cống hớn_hở trang_trọng khẳng_định chiếm_hữu trọn_vẹn con_em trang_trại_vậy quotkhông đùaquot xui lắm_điều rồ_dại đính_hôn quotlaurie chín_chắn lứa giờ_phút\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"# Apply for all .txt files in folder\n# Or apply only for 1000 files first (set num_file or set None)\n\nimport os\nimport re\nimport pandas as pd\nfrom tqdm import tqdm\nfrom num2words import num2words as n2w\nimport spacy\nfrom pyvi import ViTokenizer\nimport time\n\nnlp = spacy.load('en_core_web_sm')\n\ndef load_stopwords(filepath):\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return set(f.read().splitlines())\n\ndef load_vietnamese_proper_nouns(filepath):\n    with open(filepath, 'r', encoding='utf-8') as f:\n        return set(f.read().splitlines())\n\ndef normalize_text(text):\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[,.]', ' ', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'(\\d+)', '<number>', text)\n    return text\n\ndef remove_english_proper_nouns(text):\n    doc = nlp(text)\n    filtered_tokens = [token.text for token in doc if not (token.ent_type_ == \"PERSON\")]\n    return ' '.join(filtered_tokens)\n\ndef remove_vietnamese_proper_nouns(text, proper_nouns):\n    for noun in proper_nouns:\n        text = text.replace(noun, '')\n    return text\n\ndef normalize_vietnamese_text(text):\n    text = ViTokenizer.tokenize(text)\n    text = re.sub(r'\\u200B', '', text)\n    return text.replace(' ', '')\n\ndef lemmatize_numbers(text):\n    return re.sub(r'(\\d+)', lambda x: n2w(int(x.group())), text)\n\ndef tokenize_vietnamese_text(text):\n    return ViTokenizer.tokenize(text)\n\ndef remove_duplicates(tokens):\n    seen = set()\n    unique_tokens = []\n    for token in tokens.split():\n        if token not in seen:\n            unique_tokens.append(token)\n            seen.add(token)\n    return ' '.join(unique_tokens)\n\ndef preprocess_text(text, stop_words, proper_nouns):\n    text = normalize_text(text)\n    text = remove_english_proper_nouns(text)\n    text = remove_vietnamese_proper_nouns(text, proper_nouns)\n    tokens = tokenize_vietnamese_text(text)\n    tokens = [word for word in tokens.split() if word not in stop_words]\n    tokens = remove_duplicates(' '.join(tokens))\n    text = lemmatize_numbers(tokens)\n    return text\n\ndef process_text_file(input_file, output_file, stop_words, proper_nouns):\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    try:\n        with open(input_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        # Chia nhỏ nội dung nếu nó dài hơn 1 triệu ký tự\n        max_length = 1000000\n        if len(content) > max_length:\n            parts = [content[i:i + max_length] for i in range(0, len(content), max_length)]\n            processed_text = ''\n            for part in parts:\n                processed_text += preprocess_text(part, stop_words, proper_nouns) + '\\n'\n        else:\n            processed_text = preprocess_text(content, stop_words, proper_nouns)\n\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(processed_text)\n            \n    except Exception as e:\n        print(f\"Error processing {input_file}: {e}\")\n        raise  # Dừng xử lý và nâng lỗi\n\ninput_folder_path = '/kaggle/working/cleaned_header_footer_books'\noutput_folder_path = '/kaggle/working/preprocessed_books'\nstopwords_path = '/kaggle/input/vietnamesestopwordstxt/vietnamese-stopwords.txt'\nvietnamese_proper_nouns_path = '/kaggle/input/d/quynhnhitran02/vietnamese-proper-nouns/Vietnamese_Proper_Nouns.txt'\n\nstart_time = time.time()\n\nnum_files = 500  # Thay đổi giá trị này thành None để xử lý toàn bộ file  # 1 file thì xử lý tầm 9s, 500 file tầm 1,25h => ?_?\ninput_files = [f for f in os.listdir(input_folder_path) if f.endswith('.txt')]\n\nif num_files is not None and num_files < len(input_files):\n    input_files = input_files[:num_files]\n\nstop_words = set()\nvietnamese_proper_nouns = set()\n\nwith tqdm(total=2, desc=\"Loading resources\") as pbar:\n    stop_words = load_stopwords(stopwords_path)\n    pbar.update(1)\n    \n    vietnamese_proper_nouns = load_vietnamese_proper_nouns(vietnamese_proper_nouns_path)\n    pbar.update(1)\n\nwith tqdm(total=len(input_files), desc=\"Processing files\") as pbar:\n    for input_file in input_files:\n        input_file_path = os.path.join(input_folder_path, input_file)\n        output_file_path = os.path.join(output_folder_path, input_file)\n        \n        try:\n            process_text_file(input_file_path, output_file_path, stop_words, vietnamese_proper_nouns)\n        except Exception as e:\n            print(f\"Stopping processing due to error: {e}\")\n            break  # Dừng xử lý khi có lỗi, để check các trưởng hợp ngoại lệ, có thể pass các case ngoại lệ vì data nhiều được không?\n\n        pbar.update(1)\n\nend_time = time.time()\nprint(f\"Processing completed in {end_time - start_time:.2f} seconds.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T07:30:25.975626Z","iopub.execute_input":"2024-10-12T07:30:25.976188Z","iopub.status.idle":"2024-10-12T08:18:21.795972Z","shell.execute_reply.started":"2024-10-12T07:30:25.976132Z","shell.execute_reply":"2024-10-12T08:18:21.794567Z"}},"outputs":[{"name":"stderr","text":"Loading resources: 100%|██████████| 2/2 [00:00<00:00, 123.76it/s]\nProcessing files: 100%|██████████| 500/500 [47:54<00:00,  5.75s/it]  ","output_type":"stream"},{"name":"stdout","text":"Processing completed in 2874.58 seconds.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"processed_4cogai_path = '/kaggle/working/preprocessed_books/Những phụ nữ mở nước đâu tiên - Sử Gia Trần Gia Phụng.txt'\n\nprocessed_4cogai = read_output_file(processed_4cogai_path)\n\nprint(processed_4cogai)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T08:57:18.526365Z","iopub.execute_input":"2024-10-12T08:57:18.527221Z","iopub.status.idle":"2024-10-12T08:57:18.534422Z","shell.execute_reply.started":"2024-10-12T08:57:18.527178Z","shell.execute_reply":"2024-10-12T08:57:18.533396Z"}},"outputs":[{"name":"stdout","text":"sử_gia trần gia phụng phụ_nữ việt_nam đầu_tiên hai trưng tiểu_sử sự_nghiệp ý_kiến hầu_như nêu lịch_sử thế_giới khởi_nghĩa chống ngoại_xâm < number > công_nguyên bậc nữ lưu đứng giành độc_lập đất_nước anh_hùng jeanne d arc pháp thế_kỷ công_chúa huyền trân ngọc vạn khoa liên_kết_đẩy lui xâm_lăng quân mông cổ giao_hảo đại việt tốt_đẹp tân sửu sứ_giả phẩm vật thăm_viếng ngoại_giao thái thượng hoàng nhân tông đi xuất_gia đi_tu thăm lễ du ngoạn âm_lịch vua chế mân jaya simhavarman iv trị_vì nguyên thái_tử bổ đích harijit đầu indravarman xi thời kháng_nguyên iii già nắm trọng_trách điều_khiển chỉ_huy chiêm_đẩy lực_lượng toa đô sogatu gặp_gỡ hứa gả con_gái có_thể bang_giao chiêm bền_vững hôn_nhân nầy phản_bác triều đình ta quan_niệm khắc biệt chủng_tộc quan ngăn_trở mãi quyết châu ô bắc sính cưới diễn bính ngọ đinh mùi thành thuận lẽ lý thành_hóa châu_hóa thay_đổi dạy_dỗ ngày_nay nam tỉnh quảng_trị thừa thiên hóa quảng_nam tổng_cộng vùng_đất km phong hậu paramecvari đám_cưới từ_trần giàn_hỏa_thiêu chết chồng tục_lệ nhà_vua tướng đỗ cớ lập_mưu đa da trở thống chí quyển viết định sơn xã hổ thiên_bản phủ nghĩa hưng sách số_phận hoàng_tử sử tượng_trưng phát_triển một_cách hòa_thuận phương truyền_thống sống hy_sinh công tác_giả vô_danh đề_cao ca huế điệu nam_bình truyền_tụng nước_non ngàn dặm nợ ly đắng_cay đương độ xuân lao_đao duyên má tuyết liều hoa tàn trăng_khuyết vàng lộn chì khúc cớ_sao mường_tượng nghê chim hồng nhạn bay tình tha_thiết bóng dương nhắn đặng phân lợi dân_tình đem cân đắng_cay_trăm nguyễn sãi vương phúc tuổi_ta kế_vị đàng di_mệnh xây_dựng vững_mạnh chúa trịnh cố chân cambodia lạp lên_ngôi chey chetta kết thân lan cầu_hôn ghi tiến man_di đại_nam liệt truyện tiền biên mục khuyết truyện_tức nghĩa_là gia_phả ấn_hành gia_đình canh nhì cheychetta ii hôn cử sứ nhượng khu dinh điền mô xoài rịa vận_động canh_tác chính_thức đặt_chân đất bàn_đạp đồng_bằng sông long tục xảy báu mậu tuất hoàng_thân đánh bấy_giờ nặc thất_bại thái_hậu giúp_đỡ cầu_cứu hiền tần cháu gọi ruột liền phó_tướng tôn thất yến trấn giúp hành_quân reachea ngày_càng can_thiệp thâm_nhập sinh_sống tận mũi cà_mau đường tranh_chấp nội_bộ ngọc_khoa bốn người_lớn trẻ kết_hôn thế_nào tiểu_truyện đề may thay tộc phả hội_đồng chép rằngquot đức hy vương_gả pôrômê phối vấn_đề sâu_xa chiến_tranh miền vừa_mới bùng_nổ mão bố quảng bình phú yên văn_phong vơi nổi hữu vinh liên dẹp đổi lo_ngại lâm trạnglưỡng thọ địch đào nha macao trung thương_thuyền bồ ghé buôn_bán trao_đổi hải phan rang nguy_hiểm nhất_là pô ro mê nguyên_nhân quyết_định dàn_xếp hòa hiếu poromê nhắm rút ngòi_nổ đảm an_ninh mặt tây_phương ghi_nhận vì_sao giao_thương phải_chăng hậu_quả hoàng_hậu tám truyền_thuyết tục_ngữ trách_cứ trở_nên mê_muội sụp_đổ dân_tộc chàm dohamide dorohiem phụ_trách tháp e aymonier câu_chuyện vợ truyền gái gốc đê bia chanh_bà sinh phik chơk quotliên nhược_điểm tâm tánh yếu_đuối sắc_đẹp mỹ_nhân yuôn đẹp giả_dạng làm_khách thương léo tin_tức duyên_dáng ngoại bang tai ut nữ_hoàng mê_hoặc chặt tượng thiêng_liêng vương_quốc vì_vậy dân_chúng câu_đố quotô ngài linh_thiêng rước kinh lim ứng quotsanak jak po ginrơh patrai tok kamei ywơn lihik ngoài_ra câu thành_ngữ mỉa_mai limuk you thần_linh_hóa phản_ảnh sự_thật suy_yếu hẳn nhanh_chóng đất_đai mở_đường quả_thật mở_rộng biên_cương địa chiến_công vệ dạng luôn_luôn đầy_đủ âm_thầm chú_ý thi_sĩ pierre corneille kịch cổ_điển le cid a vaincre sans péril on triomphe gloire chiến_thắng không_gian_nguy khải hoàn vinh_dự tuy_nhiên êm_đềm tốn xương_máu can_đảm thực_hiện chú_thích nghi_lễ vợ_hỏa_thiêu ấn_giáo trà tỳ suttee thịnh_hành ấn bãi_bỏ sử_ký toàn thư chữ_nho hà dịch nxb khoa_học xã_hội tập tr quốc quán viện hoá tái_bản phạm văn lê thời_đại sài_gòn tt trị_sự nx thuận_hóa trọng vnsl bỏ_qua xứ ngọc_vạn quotviệc có_lẽ thần giấu cẩn án đảnh sự_tích chồng_con nàng tức_là houston thu hạ đánh_số trang xuyên suốt q như_vậy dựa tấu trình cư trinh phước đồng nai quotcuộc hưởng nguyễnquot đăng phục tiếp trị sđd vùng_biên_giới biên_giới vùng_biên mạc cảnh cảnh_huống the relations between champa and southeast asiaquot proceedings of seminar thuyết_trình hội_thảo quốc_tế đại_học copenhagen đan mạch nguyên_bản hội nghiên_cứu đảo đông paris giáo tế california asia community resource center tắt sacrc cai_trị thời_gian quotvới tô thoại ngô doanh chămpa ampamp huyền_thoại hà_nội văn_hóa thông_tin inrasara chăm insara trạm\n","output_type":"stream"}],"execution_count":139},{"cell_type":"markdown","source":"archieve data after preprocess","metadata":{}},{"cell_type":"code","source":"# Tạo folder để lưu các folder được nén => save nhanh hơn\n\noutput_folder = '/kaggle/working/save'\nos.makedirs(output_folder, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T08:36:28.140988Z","iopub.execute_input":"2024-10-12T08:36:28.141388Z","iopub.status.idle":"2024-10-12T08:36:28.146938Z","shell.execute_reply.started":"2024-10-12T08:36:28.141348Z","shell.execute_reply":"2024-10-12T08:36:28.145879Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"input_folder = '/kaggle/working/cleaned_header_footer_books'\n\noutput_zip = os.path.join(output_folder, 'cleaned_header_footer_books.zip')\n\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', input_folder)\n\nprint(f\"Folder {input_folder} has been zipped to {output_zip}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T08:37:02.637285Z","iopub.execute_input":"2024-10-12T08:37:02.637705Z","iopub.status.idle":"2024-10-12T08:38:58.407273Z","shell.execute_reply.started":"2024-10-12T08:37:02.637665Z","shell.execute_reply":"2024-10-12T08:38:58.405708Z"}},"outputs":[{"name":"stdout","text":"Folder /kaggle/working/cleaned_header_footer_books has been zipped to /kaggle/working/save/cleaned_header_footer_books.zip\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"# Zip folder preprocessed_books\n\ninput_folder = '/kaggle/working/preprocessed_books'\n\noutput_zip = os.path.join(output_folder, 'preprocessed_books.zip')\n\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', input_folder)\n\nprint(f\"Folder {input_folder} has been zipped to {output_zip}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T08:41:39.798352Z","iopub.execute_input":"2024-10-12T08:41:39.798775Z","iopub.status.idle":"2024-10-12T08:41:40.461309Z","shell.execute_reply.started":"2024-10-12T08:41:39.798735Z","shell.execute_reply":"2024-10-12T08:41:40.460219Z"}},"outputs":[{"name":"stdout","text":"Folder /kaggle/working/preprocessed_books has been zipped to /kaggle/working/save/preprocessed_books.zip\n","output_type":"stream"}],"execution_count":132},{"cell_type":"markdown","source":"Chuẩn bị input cho model","metadata":{}},{"cell_type":"code","source":"# Step 1: Chuẩn bị dl\n\n# Chọn ra 500 file .txt có dung lượng lớn nhất => Đảm bảo 500 file này có nội dung đủ nhiều và phong phú cho model học (Test trước với 500 files)\n\ninput_dir = '/kaggle/working/preprocessed_books'\nfiles = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.txt')]\nlargest_files = sorted(files, key=os.path.getsize, reverse=True)[:500]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load file\ntexts = []\nfor file in tqdm(largest_files, desc=\"Loading text files\"):\n    with open(file, 'r', encoding='utf-8') as f:\n        texts.append(f.read())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.256913Z","iopub.status.idle":"2024-10-12T13:52:25.257284Z","shell.execute_reply.started":"2024-10-12T13:52:25.257101Z","shell.execute_reply":"2024-10-12T13:52:25.257120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts[:1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.258821Z","iopub.status.idle":"2024-10-12T13:52:25.259210Z","shell.execute_reply.started":"2024-10-12T13:52:25.259010Z","shell.execute_reply":"2024-10-12T13:52:25.259027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train model\n(Phần này em tham khảo từ https://github.com/Narius2030/Vietnamese-Text-Generator/blob/main/text-generator-train.ipynb)\n- Em bị crash ở phần train model, model.fit. Mặc dù đã thử đổi batch size = 32, 64, 128 hay 256, tăng và giảm epoch, giảm số files từ 50->100. Nhưng vẫn bị crash :<\n- Accelerator ở Kaggle em chọn GPU T4 x2.","metadata":{}},{"cell_type":"code","source":"# filter several punctuations in texts\ntokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!“\"”#$%&()*+,-./:;<=>?@[\\]^`{|}~ ')\ntokenizer.fit_on_texts(texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.262122Z","iopub.status.idle":"2024-10-12T13:52:25.262447Z","shell.execute_reply.started":"2024-10-12T13:52:25.262282Z","shell.execute_reply":"2024-10-12T13:52:25.262299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the length of the vocabulary\nprint(len(tokenizer.word_index))\ntokenizer.word_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.263749Z","iopub.status.idle":"2024-10-12T13:52:25.264094Z","shell.execute_reply.started":"2024-10-12T13:52:25.263923Z","shell.execute_reply":"2024-10-12T13:52:25.263940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tạo batch sequences từ dữ liệu\ndef create_input_generator(text_data, batch_size=10000):\n    for i in range(0, len(text_data), batch_size):\n        batch_texts = text_data[i:i + batch_size]\n        input_sequences = []\n        for line in batch_texts:\n            token_list = tokenizer.texts_to_sequences([line])[0]\n            for j in range(1, len(token_list)):\n                n_gram_sequence = token_list[:j+1]\n                input_sequences.append(n_gram_sequence)\n        yield input_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.265427Z","iopub.status.idle":"2024-10-12T13:52:25.265832Z","shell.execute_reply.started":"2024-10-12T13:52:25.265616Z","shell.execute_reply":"2024-10-12T13:52:25.265643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generator sẽ tạo batch các sequences để giảm tải bộ nhớ\ndef create_sequences_in_batches(text_data, batch_size=10000):\n    sequences_digit = []\n    for batch in create_input_generator(text_data, batch_size):\n        sequences_digit.extend(batch)\n    return sequences_digit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.268614Z","iopub.status.idle":"2024-10-12T13:52:25.268973Z","shell.execute_reply.started":"2024-10-12T13:52:25.268802Z","shell.execute_reply":"2024-10-12T13:52:25.268820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sử dụng batch size để tránh crash\nbatch_size = 10000\nsequences_digit = create_sequences_in_batches(texts, batch_size)\nprint(sequences_digit[0:5])\nprint(len(sequences_digit))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T13:52:25.270623Z","iopub.status.idle":"2024-10-12T13:52:25.270981Z","shell.execute_reply.started":"2024-10-12T13:52:25.270809Z","shell.execute_reply":"2024-10-12T13:52:25.270827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Padding các sequences để có cùng chiều dài\nmax_sequence_len = 500\ninput_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(sequences_digit, maxlen=max_sequence_len, padding='pre'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Xác định kích thước từ vựng\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary size:\", vocab_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:18.635217Z","iopub.execute_input":"2024-10-12T11:02:18.636176Z","iopub.status.idle":"2024-10-12T11:02:18.642362Z","shell.execute_reply.started":"2024-10-12T11:02:18.636119Z","shell.execute_reply":"2024-10-12T11:02:18.641113Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 77812\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Tách dữ liệu thành X (input) và y (output)\nX = input_sequences[:,:-1]\ny = input_sequences[:,-1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:18.644140Z","iopub.execute_input":"2024-10-12T11:02:18.645132Z","iopub.status.idle":"2024-10-12T11:02:18.652393Z","shell.execute_reply.started":"2024-10-12T11:02:18.645094Z","shell.execute_reply":"2024-10-12T11:02:18.651462Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Chuyển y thành dạng one-hot\ny = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:18.653467Z","iopub.execute_input":"2024-10-12T11:02:18.653772Z","iopub.status.idle":"2024-10-12T11:02:19.707020Z","shell.execute_reply.started":"2024-10-12T11:02:18.653725Z","shell.execute_reply":"2024-10-12T11:02:19.706129Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Kiểm tra dữ liệu X và y\nprint(\"Shape of X:\", X.shape)\nprint(\"Shape of y:\", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:19.708330Z","iopub.execute_input":"2024-10-12T11:02:19.708695Z","iopub.status.idle":"2024-10-12T11:02:19.714016Z","shell.execute_reply.started":"2024-10-12T11:02:19.708658Z","shell.execute_reply":"2024-10-12T11:02:19.713098Z"}},"outputs":[{"name":"stdout","text":"Shape of X: (379437, 499)\nShape of y: (379437, 77812)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:19.715171Z","iopub.execute_input":"2024-10-12T11:02:19.715528Z","iopub.status.idle":"2024-10-12T11:02:19.724318Z","shell.execute_reply.started":"2024-10-12T11:02:19.715493Z","shell.execute_reply":"2024-10-12T11:02:19.723262Z"}},"outputs":[{"name":"stdout","text":"[[    0     0     0 ...     0     0  1048]\n [    0     0     0 ...     0  1048   747]\n [    0     0     0 ...  1048   747  1124]\n ...\n [ 1147  3241 25807 ...  5433  6848  1422]\n [ 3241 25807  1510 ...  6848  1422 77811]\n [25807  1510   814 ...  1422 77811 12598]]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print('The length of a one-hot vector y: ', len(y[0]))\nprint('The collection of y vectors for all sentences', y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:19.725305Z","iopub.execute_input":"2024-10-12T11:02:19.725690Z","iopub.status.idle":"2024-10-12T11:02:19.740822Z","shell.execute_reply.started":"2024-10-12T11:02:19.725652Z","shell.execute_reply":"2024-10-12T11:02:19.739840Z"}},"outputs":[{"name":"stdout","text":"The length of a one-hot vector y:  77812\nThe collection of y vectors for all sentences [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Xây dựng mô hình LSTM\nmodel = tf.keras.models.Sequential()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:19.741868Z","iopub.execute_input":"2024-10-12T11:02:19.742167Z","iopub.status.idle":"2024-10-12T11:02:19.751440Z","shell.execute_reply.started":"2024-10-12T11:02:19.742136Z","shell.execute_reply":"2024-10-12T11:02:19.750599Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Xây dựng mô hình LSTM\nmodel = tf.keras.models.Sequential()\n\n# Embedding layer\nmodel.add(tf.keras.layers.Embedding(vocab_size, 50, input_length=max_sequence_len-1))\n\n# Batch normalization và LSTM layers\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.LSTM(512, return_sequences=True))\nmodel.add(tf.keras.layers.LSTM(512))\n\n# Dense layers\nmodel.add(tf.keras.layers.Dense(100, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:19.752573Z","iopub.execute_input":"2024-10-12T11:02:19.752907Z","iopub.status.idle":"2024-10-12T11:02:20.654958Z","shell.execute_reply.started":"2024-10-12T11:02:19.752862Z","shell.execute_reply":"2024-10-12T11:02:20.653801Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Compile mô hình\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:20.656180Z","iopub.execute_input":"2024-10-12T11:02:20.656515Z","iopub.status.idle":"2024-10-12T11:02:20.675682Z","shell.execute_reply.started":"2024-10-12T11:02:20.656480Z","shell.execute_reply":"2024-10-12T11:02:20.674867Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Train model\nsummary = model.fit(X, y, batch_size=128, epochs=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T11:02:20.676737Z","iopub.execute_input":"2024-10-12T11:02:20.677044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(seed_text, next_words, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict(token_list, verbose=0)\n        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n        output_word = tokenizer.index_word.get(predicted_word_index, \"\")\n        seed_text += \" \" + output_word\n    return seed_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_text = \"Thời gian\"\nnext_words = 10\ngenerated_text = generate_text(seed_text, next_words, max_sequence_len)\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}