{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKlvFFvAlvMp"
      },
      "source": [
        "Preprocess a given text dataset and report the steps as well as the results.\n",
        "Dataset below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqeWvKu1l28u"
      },
      "source": [
        "Kaggle: https://www.kaggle.com/datasets/haitranquangofficial/vietnamese-online-news-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4lY2nEB2Lsr"
      },
      "source": [
        "Các bước xử lý:\n",
        "- Tokenizer: Tách văn bản thành các token.\n",
        "- Stop words: Loại bỏ stop words.\n",
        "- Lemmatization: Chuyển về từ gốc.\n",
        "- Normalization: Loại bỏ các ký tự đặc biệt, chữ số, dấu câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs7B6Tks2Brm"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0gMV2dm3sTT"
      },
      "outputs": [],
      "source": [
        "# download\n",
        "nltk.download('punkt') # tokenization\n",
        "nltk.download('stopwwords') # loại bỏ stopwords\n",
        "nltk.download('wordnet') # lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVLpCRahlunJ"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "\n",
        "text = 'Gần đây, Thứ trưởng Bộ Phát triển Kỹ thuật số, Truyền thông và Truyền thông đại chúng Nga - Oleg Ivanov cho biết trong một cuộc phỏng vấn rằng, đến năm 2025 Viện Khoa học & Công nghệ Skolkovo và đài phát thanh trực thuộc Bộ Kỹ thuật số Nga, Viện Khoa học Chế tạo sẽ nhận được khoản phân bổ 30 tỷ rúp để thúc đẩy các dự án chung về thiết bị liên lạc di động thế hệ thứ 6 (6G). Dự kiến, thiết bị 6G có thể được chế tạo trước năm 2025. Điều đó có nghĩa là giới chức Nga không còn quan tâm đến việc phát triển công nghệ 5G nữa mà đặt cược vốn và nguồn nhân lực của mình vào công nghệ 6G. Cách tiếp cận này rất táo bạo bởi nhìn chung, công nghệ truyền thông hiện đại cần phải tích lũy công nghệ từ thế hệ này sang thế hệ khác và sự phát triển giữa các thế hệ thường rất khó khăn. Điều khó khăn hơn nữa là 6G hiện đang trong giai đoạn nghiên cứu ban đầu trên khắp thế giới, các tiêu chuẩn kỹ thuật liên quan vẫn chưa được xác định và việc phát triển sản phẩm vẫn còn rất xa. Do đó, giới chức Nga vẫn chưa công bố thông tin chi tiết cụ thể. Ngay cả khi Nga có thể phát triển sớm mạng lưới 6G, quốc gia này có thể sớm bị tụt hậu vì các quốc gia khác sẽ nhanh chóng bắt kịp. Nhưng điều quan trọng nhất là các nhà mạng tại Nga tin rằng nhiều người tiêu dùng nước này vẫn chưa sẵn sàng nâng cấp mạng di động vì đối với họ mạng 4G là quá đủ. Các nhà mạng này thậm chí còn không bị làm phiền bởi mạng 5G chứ chưa nói đến mạng 6G còn rất lâu mới hoàn thiện.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESfk0CRV18hN"
      },
      "outputs": [],
      "source": [
        "# Normalization\n",
        "\n",
        "def normalize_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\s', ' ', text) # Xóa khoảng trắng thừa. \\s: bất kì khoảng trắng nào (cả dấu cách, tab, hoặc xuống dòng). '+': tìm một hoặc nhiều ký tự liên tiếp\n",
        "  text = re.sub(r'[^\\w\\s]', '', text) # Loại bỏ dấu câu. [^]: phủ định.\n",
        "  return text\n",
        "\n",
        "text = normalize_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b59dWqsR2hVf"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVAaVzpJ2iPN"
      },
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "# Data tiếng việt => tạo danh sách stop words riêng:\n",
        "stop_words = {\n",
        "    \"và\", \"là\", \"của\", \"nhưng\", \"hoặc\", \"một\", \"có\", \"cho\", \"về\", \"này\", \"khi\", \"ở\", \"được\", \"với\",\n",
        "    \"tại\", \"cả\", \"các\", \"những\", \"rằng\", \"thì\", \"lại\", \"đã\", \"sẽ\", \"đang\", \"tôi\", \"anh\", \"em\", \"bạn\",\n",
        "    \"chúng\", \"họ\", \"như\", \"vì\", \"do\", \"trên\", \"dưới\", \"đây\", \"đó\", \"điều\", \"gì\", \"nào\", \"ra\", \"nên\",\n",
        "    \"rồi\", \"không\", \"phải\", \"nếu\", \"thì\", \"làm\", \"sao\", \"có thể\", \"nữa\", \"thế\", \"cùng\", \"chỉ\", \"mà\",\n",
        "    \"vậy\", \"nhỉ\", \"à\", \"ừ\", \"ừm\", \"đi\", \"đến\", \"hơn\", \"để\", \"trong\", \"lên\", \"xuống\", \"ngày\", \"tháng\",\n",
        "    \"năm\", \"vẫn\", \"một số\", \"bởi\", \"từ\", \"qua\", \"lúc\", \"sau\", \"trước\", \"như vậy\", \"tất cả\", \"vừa\",\n",
        "    \"chưa\", \"thế nào\", \"nhận\", \"đó là\", \"ai\", \"cái\", \"nó\", \"mỗi\", \"lại\", \"kể\", \"này\", \"ấy\", \"gì\",\n",
        "    \"vậy là\", \"liên quan\", \"dùng\", \"đúng\", \"cũng\", \"vẫn còn\", \"khác\", \"cùng với\", \"do đó\", \"vì vậy\",\n",
        "    \"giữa\", \"bao nhiêu\", \"hết\", \"nhiều\", \"ít\", \"bao giờ\", \"ở đâu\", \"tại sao\", \"thế nào\", \"ra sao\",\n",
        "    \"bao lâu\", \"càng\", \"ngay\", \"với lại\", \"hoàn toàn\", \"vẫn luôn\", \"thế\", \"bấy lâu\", \"cho nên\", \"nhiều khi\",\n",
        "    \"rõ ràng\", \"dù sao\", \"có gì\", \"nói\", \"người\", \"được\", \"như thế\", \"nhiều nhất\", \"chưa từng\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD6TNAnU3jTc"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-nu50v73PSn"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "\n",
        "# Tách từ\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Loại bỏ từ dừng\n",
        "tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5dBkxOB4sm7"
      },
      "outputs": [],
      "source": [
        "# !pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8aoZycg3TNo"
      },
      "outputs": [],
      "source": [
        "# lemmatization:\n",
        "\n",
        "# Dùng thư viên underthesea để thực hiện lemmatize với tiếng việt: pip install underthesea\n",
        "from underthesea import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRN9oIwI4yHV"
      },
      "outputs": [],
      "source": [
        "# Thực hiện lemmatization\n",
        "lemmatized_tokens = []\n",
        "for token in tokens:\n",
        "    # pos_tag => lấy dạng gốc của từ\n",
        "    pos_tags = pos_tag(token)\n",
        "    # pos_tags => tuple: (từ, nhãn từ loại)\n",
        "    lemma = pos_tags[0][0]  # Lấy từ gốc\n",
        "    lemmatized_tokens.append(lemma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzZQ80fd5EbY"
      },
      "outputs": [],
      "source": [
        "# Nối các tokens lại thành 1 text\n",
        "text_from_tokens = ' '.join(tokens)\n",
        "print(\"Text from Tokens:\")\n",
        "text_from_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF12bnBF5LWy"
      },
      "outputs": [],
      "source": [
        "# Nối các lemmatized_tokens lại thành 1 text\n",
        "text_from_lemmatized = ' '.join(lemmatized_tokens)\n",
        "print(\"Text from Lemmatized Tokens:\")\n",
        "text_from_lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IWLQrgR2Pj7"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "with open('../data/w2/news_dataset.json', 'r', encoding='utf-8') as file:\n",
        "    dataset = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVF6Agl91yzR"
      },
      "outputs": [],
      "source": [
        "# Apply for all dataset\n",
        "\n",
        "def process_data(news):\n",
        "    # Normalization\n",
        "    content = normalize_text(article['content'])\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(content)\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = []\n",
        "    for token in tokens:\n",
        "        pos_tags = pos_tag(token)\n",
        "        lemma = pos_tags[0][0]\n",
        "        lemmatized_tokens.append(lemma)\n",
        "\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "# Save data\n",
        "processed_dataset = []\n",
        "\n",
        "for article in dataset:\n",
        "    processed_article = article.copy()\n",
        "    processed_article['content'] = process_data(article)\n",
        "    processed_dataset.append(processed_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1gzVCtf2yW4"
      },
      "outputs": [],
      "source": [
        "processed_dataset.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMFDLoL2zon"
      },
      "outputs": [],
      "source": [
        "# # Save  back to a new JSON file\n",
        "# with open('processed_news_dataset.json', 'w', encoding='utf-8') as file:\n",
        "#     json.dump(processed_dataset, file, ensure_ascii=False, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
